{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iac: Create S3 Bucket and Launch Redshift Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import configparser\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Configuration Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('airflow/config/aws.cfg'))\n",
    "\n",
    "KEY                    = config.get('AWS','KEY')\n",
    "SECRET                 = config.get('AWS','SECRET')\n",
    "BUCKET                 = config.get('AWS','BUCKET')\n",
    "REGION                 = config.get('AWS', 'REGION')\n",
    "\n",
    "DWH_CLUSTER_TYPE       = config.get(\"DWH\", \"DWH_CLUSTER_TYPE\")\n",
    "DWH_NUM_NODES          = config.get(\"DWH\",\"DWH_NUM_NODES\")\n",
    "DWH_NODE_TYPE          = config.get(\"DWH\",\"DWH_NODE_TYPE\")\n",
    "\n",
    "DWH_CLUSTER_IDENTIFIER = config.get(\"DWH\",\"DWH_CLUSTER_IDENTIFIER\")\n",
    "DWH_DB                 = config.get(\"DWH\",\"DWH_DB\")\n",
    "DWH_DB_USER            = config.get(\"DWH\",\"DWH_DB_USER\")\n",
    "DWH_DB_PASSWORD        = config.get(\"DWH\",\"DWH_DB_PASSWORD\")\n",
    "DWH_PORT               = config.get(\"DWH\",\"DWH_PORT\")\n",
    "\n",
    "DWH_IAM_ROLE_NAME      = config.get(\"DWH\", \"DWH_IAM_ROLE_NAME\")\n",
    "\n",
    "(DWH_DB_USER, DWH_DB_PASSWORD, DWH_DB)\n",
    "\n",
    "pd.DataFrame({\"Param\":\n",
    "                  [\"DWH_CLUSTER_TYPE\", \"DWH_NUM_NODES\", \"DWH_NODE_TYPE\", \"DWH_CLUSTER_IDENTIFIER\", \"DWH_DB\", \"DWH_DB_USER\", \"DWH_DB_PASSWORD\", \"DWH_PORT\", \"DWH_IAM_ROLE_NAME\"],\n",
    "              \"Value\":\n",
    "                  [DWH_CLUSTER_TYPE, DWH_NUM_NODES, DWH_NODE_TYPE, DWH_CLUSTER_IDENTIFIER, DWH_DB, DWH_DB_USER, DWH_DB_PASSWORD, DWH_PORT, DWH_IAM_ROLE_NAME]\n",
    "             })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instaniate AWS Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec2 = boto3.resource('ec2',\n",
    "                     aws_access_key_id=KEY,\n",
    "                     aws_secret_access_key=SECRET,\n",
    "                     region_name=REGION\n",
    "                    )\n",
    "\n",
    "s3 = boto3.resource('s3',\n",
    "                    aws_access_key_id=KEY,\n",
    "                    aws_secret_access_key=SECRET,\n",
    "                    region_name=REGION\n",
    "                   )\n",
    "\n",
    "iam = boto3.client('iam',\n",
    "                   aws_access_key_id=KEY,\n",
    "                   aws_secret_access_key=SECRET,\n",
    "                   region_name=REGION\n",
    "                  )\n",
    "\n",
    "redshift = boto3.client('redshift',\n",
    "                        aws_access_key_id=KEY,\n",
    "                        aws_secret_access_key=SECRET,\n",
    "                        region_name=REGION\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create AWS S3 Sample Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create s3 bucket\n",
    "try:\n",
    "    s3.create_bucket(Bucket=BUCKET, CreateBucketConfiguration={'LocationConstraint': REGION})\n",
    "except Exception as e:\n",
    "    print('Bucket Already Exists')\n",
    "\n",
    "# local path to sample_data\n",
    "local_path = 'data/'\n",
    "\n",
    "#for file in local_path, add to s3 bucket\n",
    "file_count = 0\n",
    "for root,dirs,files in os.walk(local_path):\n",
    "    for file in files:\n",
    "        local_file_path = os.path.join(root,file)\n",
    "        bucket_file_path = os.path.join(root.replace(local_path,''),file)\n",
    "        s3.Object(BUCKET, bucket_file_path).put(Body=open(local_file_path, 'rb'))\n",
    "\n",
    "        file_count += 1\n",
    "        if file_count % 10 == 0:\n",
    "            print('Files Uploaded: {}'.format(file_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list files within s3 bucket\n",
    "bucket = s3.Bucket(BUCKET)\n",
    "for key in bucket.objects.all():\n",
    "    print(key.key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Role "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before creating the role, make sure the AWS user defined in the aws.cfg has permission to create roles and attach policies or has administrative access. For the sake of simplicity, using a user with administrative access would be ideal. The cell below is for reference given a scenario where explicit role policies are needed to be given to specific user(s). The admin user would have to enter their key and secret values in the cell below. The cell will overide the admin variable with an empty value after the policies are attached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admin = boto3.client('iam',\n",
    "                   aws_access_key_id='',\n",
    "                   aws_secret_access_key='',\n",
    "                   region_name=REGION\n",
    "                  )\n",
    "                  \n",
    "user_role_policy = json.dumps(\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"iam:AttachRolePolicy\",\n",
    "                \"iam:CreateRole\",\n",
    "                \"iam:PutRolePolicy\",\n",
    "                \"iam:GetRole\",\n",
    "                \"iam:DetachRolePolicy\",\n",
    "                \"iam:PassRole\"\n",
    "            ],\n",
    "             \"Resource\": f\"arn:aws:s3:::{BUCKET}\"\n",
    "        }\n",
    "    ]\n",
    "})\n",
    "\n",
    "try:\n",
    "    user_role_arn = admin.create_policy(\n",
    "    PolicyName='RolePolicy',\n",
    "    Path='/',\n",
    "    PolicyDocument=user_role_policy,\n",
    "    Description='Allows user to manage roles'\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "response = admin.attach_user_policy(\n",
    "    UserName=DWH_DB_USER,\n",
    "    PolicyArn=user_role_arn['Policy']['Arn']\n",
    ")\n",
    "\n",
    "user_policy_list = ['arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess', 'arn:aws:iam::aws:policy/AmazonRedshiftQueryEditor', 'arn:aws:iam::aws:policy/AmazonRedshiftFullAccess']\n",
    "for policy in user_policy_list:\n",
    "    admin.attach_user_policy(\n",
    "        UserName=DWH_DB_USER, \n",
    "        PolicyArn=policy\n",
    "    )\n",
    "\n",
    "# overite admin variable for security purposes\n",
    "admin = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_trust_policy = json.dumps(\n",
    "{'Statement': [{'Action': 'sts:AssumeRole',\n",
    "               'Effect': 'Allow',\n",
    "               \"Action\": \"sts:AssumeRole\",\n",
    "               'Principal': {'Service': 'redshift.amazonaws.com', \"AWS\": f\"arn:aws:iam::501460770806:user/{DWH_DB_USER}\"}}],\n",
    "               'Version': '2012-10-17'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_trust_policy = json.dumps(\n",
    "{'Statement': [{'Action': 'sts:AssumeRole',\n",
    "               'Effect': 'Allow',\n",
    "               \"Action\": \"sts:AssumeRole\",\n",
    "               'Principal': {'Service': 'redshift.amazonaws.com', \"AWS\": f\"arn:aws:iam::501460770806:user/{DWH_DB_USER}\"}}],\n",
    "               'Version': '2012-10-17'}\n",
    ")\n",
    "\n",
    "# create IAM role\n",
    "print(\"Creating Role\")\n",
    "try:\n",
    "    role = iam.create_role(\n",
    "        Path='/',\n",
    "        RoleName=DWH_IAM_ROLE_NAME,\n",
    "        Description = \"Allows Redshift clusters to call AWS services on your behalf.\",\n",
    "        AssumeRolePolicyDocument=role_trust_policy\n",
    "    )    \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "print(\"Attaching Policy\")\n",
    "\n",
    "response = iam.attach_role_policy(RoleName=DWH_IAM_ROLE_NAME,\n",
    "                       PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    "                      )['ResponseMetadata']['HTTPStatusCode']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or Resume Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = redshift.create_cluster(    \n",
    "        #Redshift cluster config    \n",
    "        ClusterType=DWH_CLUSTER_TYPE,\n",
    "        NodeType=DWH_NODE_TYPE,\n",
    "        NumberOfNodes=int(DWH_NUM_NODES),\n",
    "\n",
    "        #Identifiers & Credentials\n",
    "        DBName=DWH_DB,\n",
    "        ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,\n",
    "        MasterUsername=DWH_DB_USER,\n",
    "        MasterUserPassword=DWH_DB_PASSWORD,\n",
    "        \n",
    "        #Roles (for s3 access)\n",
    "        IamRoles=[roleArn]  \n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resume cluster\n",
    "try:\n",
    "    redshift.resume_cluster(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prettyRedshiftProps(props):\n",
    "    \"\"\"Returns redshift cluster properties\n",
    "    Keyword Argument:\n",
    "    props -- Cluster property dictionary  (redshift variable called with describe_clusters attribute)\n",
    "    \"\"\"\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId']\n",
    "    x = [(k, v) for k,v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])\n",
    "\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "# prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell will print out when cluster is available\n",
    "cluster_status = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]['ClusterStatus']\n",
    "while cluster_status != 'available':\n",
    "    time.sleep(60)\n",
    "    cluster_status = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]['ClusterStatus']\n",
    "else:\n",
    "    print('Cluster is Available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN CELL ONLY WHEN CLUSTER IS AVAILABLE\n",
    "DWH_ENDPOINT = myClusterProps['Endpoint']['Address']\n",
    "DWH_ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add cluster endpoint to aws.cfg for airflow connection setup (connections.sh)\n",
    "with open('airflow/config/aws.cfg', 'a') as cfg:\n",
    "    cfg.write('DWH_HOST=' + DWH_ENDPOINT + '\\n')\n",
    "    cfg.write('DWH_ROLE_ARN=' + DWH_ROLE_ARN + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allow Inbound TCP port to Access Redshift Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    vpc = ec2.Vpc(id=myClusterProps['VpcId'])\n",
    "    defaultSg = list(vpc.security_groups.all())[0]\n",
    "    print(defaultSg)\n",
    "    defaultSg.authorize_ingress(\n",
    "        GroupName=defaultSg.group_name,\n",
    "        # GroupName='default',\n",
    "        CidrIp='0.0.0.0/0',\n",
    "        IpProtocol='TCP',\n",
    "        FromPort=int(DWH_PORT),\n",
    "        ToPort=int(DWH_PORT)\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Postgres Database for Airflow Backend\n",
    "\n",
    "Task parrelization isn't available with the default sqlite backend and sequential executor. A postgres database for the airflow backend will allow the local executor to be used for task parrelization.\n",
    "\n",
    "Instructions:\n",
    "- [Download Postgres UI App](https://www.postgresql.org/download/)\n",
    "- Within Postgres query editor or psql terminal, run: CREATE DATABASE database_name;\n",
    "- If you created the database with another user other than the default postgres user, add username to POSTGRES_USER below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment cell below to install sql magic\n",
    "# ! pip install ipython-sql\n",
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSTGRES_USER = \n",
    "POSTGRES_PASSWORD = \n",
    "POSTGRES_HOST = 'postgres'\n",
    "POSTGRES_PORT = '5432'\n",
    "POSTGRES_DB = \n",
    "\n",
    "postgres_conn_string = \"postgresql://{}:{}@{}:{}/{}\".format(POSTGRES_USER, POSTGRES_PASSWORD, POSTGRES_HOST, POSTGRES_PORT, POSTGRES_DB)\n",
    "\n",
    "config = configparser.ConfigParser(allow_no_value=True)\n",
    "config.read_file(open('airflow/config/airflow.cfg'))\n",
    "\n",
    "#update to postgres string\n",
    "config.set('core','sql_alchemy_conn', postgres_conn_string)\n",
    "#write changes to config file\n",
    "with open('airflow/config/airflow.cfg', 'w') as configfile:\n",
    "    config.write(configfile)\n",
    "    configfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postgres_conn_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql $postgres_conn_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Redshift Relational Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redshift_conn_string=\"postgresql://{}:{}@{}:{}/{}\".format(DWH_DB_USER, DWH_DB_PASSWORD, DWH_ENDPOINT, DWH_PORT,DWH_DB)\n",
    "%sql $redshift_conn_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix STL Error Table\n",
    "\n",
    "In the process of running the data pipeline, the staging tasks may raise stl load errors. The DAG handles these errors by transfering error rows to separate error table for each staging table that raises stl load errors. The staging tasks will still continue to load non-error rows to the designated tables, although the downstream staging_success_check tasks will fail to prevent the fact tables to be created with data integrity issues. \n",
    "\n",
    "Use the template code below to: fix any stl errors within the error tables, insert the fixed error tables into the related staging tables, and lastly drop the error table from the redshift data warehouse. After the error table is dropped, head back to the Airflow UI and re-run the staging_success_check task associated with the fixed error table. Re-running the staging_success_check will label the task as success to allow the downstream fact table tasks to be executed. After re-running the staging_success_check task, if the task is still labeled as failed, use the cells below to make sure the error table is dropped from Redshift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM stl_load_errors LIMIT 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use for debugging error table\n",
    "%sql SELECT * FROM data_science_video_log_errors LIMIT 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_table = 'data_science_video_log_errors'\n",
    "table = 'data_science_video_log'\n",
    "\n",
    "update_sql = f\"\"\"UPDATE {error_table}\n",
    "SET \n",
    "\"\"\"\n",
    "\n",
    "insert_sql = f\"\"\"\n",
    "INSERT INTO \n",
    "    {table}\n",
    "SELECT \n",
    "/*  cast error columns to staging table data type */\n",
    "FROM \n",
    "    {error_table}\n",
    "\"\"\"\n",
    "\n",
    "drop_sql = f\"DROP TABLE {error_table}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Fixing rows within {error_table}')\n",
    "%sql $update_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Inserting fixed rows into {table}')\n",
    "%sql $insert_sql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Dropping {error_table}')\n",
    "%sql $drop_sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM projects_dim LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM users_dim LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM videos_dim LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Staging Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM data_science_project_feedback WHERE EXTRACT(MONTH FROM submit_date) = 2 LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM data_science_section_feedback LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM data_science_video_log LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM data_science_mentor_activity LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fact Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM data_science_highest_prompt_score LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM data_science_highest_answer_score LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM data_science_avg_video_views_per_user LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql select * FROM data_science_avg_video_view_range LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete or Pause Resources\n",
    "\n",
    "For PostgreSQL connection, shut down server throught PostgreSQL app or psql terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pause cluster\n",
    "try:\n",
    "    pause_cluster = redshift.pause_cluster(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete cluster\n",
    "# response = redshift.delete_cluster(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER, SkipFinalClusterSnapshot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)[prettyRedshiftProps(myClusterProps)['Key'] == 'ClusterStatus']['Value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete IAM role\n",
    "iam.detach_role_policy(RoleName=DWH_IAM_ROLE_NAME, PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\")\n",
    "iam.delete_role(RoleName=DWH_IAM_ROLE_NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('airflow': conda)",
   "language": "python",
   "name": "python38264bitairflowconda241aa32da08d477cb1d76686edce0c4e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
