{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Data Engineering Capstone Project\n",
    "\n",
    "Automated Airflow data pipeline that loads data from AWS S3 to AWS Redshift, performs analytic queries, and creates easily accessible tables needed by the data team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Project Scope\n",
    "For this project, I wanted to focus on creating data pipelines for online education analytics. Being that I'm enrolled in an online education program, I was hoping to access Udacity's user data for this project. Although after asking the udacity support team, they politely declined my request. As for plan b, I decided to create my own synthetic Udacity data and use unrelated publicly accesible text data (creating unique large scale Udacity text data would be a time-killer).  Given that this project revolves around the data pipeline process, data integrity isn't as important.\n",
    "\n",
    "## Datasets\n",
    "\n",
    "All data files that would be used for fact tables were partitioned by nanodegree name, year and month. \n",
    "\n",
    "\n",
    "\n",
    "Example S3 path:\n",
    "\n",
    "All data files that would be used for dimensional tables were stored in one directory. \n",
    "\n",
    "Example S3 path:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\t\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "df_spark =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#write to parquet\n",
    "df_spark.write.parquet(\"sas_data\")\n",
    "df_spark=spark.read.parquet(\"sas_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dag diagram\n",
    "\n",
    "expalin parameterization \n",
    "easily add new degrees with degree list\n",
    "\n",
    "degree task are easily parrelized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Airflow Subdags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Staging Subdag\n",
    "\n",
    "### Tasks\n",
    "\n",
    "Create Redshift Table\n",
    "\n",
    "Example\n",
    "Load S3 data to Redshift Table\n",
    "\n",
    "Data Quality Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subdag pic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Data Dictionary \n",
    "\n",
    "Mentor Activity:\n",
    "\n",
    "Video Log:\n",
    "\n",
    "Project Feedback:\n",
    "\n",
    "Section Feedback:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "db diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytics Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Why technologies:\n",
    "\n",
    "Airflow:\n",
    "AWS S3:\n",
    "AWS Redshift:\n",
    "\n",
    "Data Pipeline Scheduling:\n",
    "monthly\n",
    "maybe weekly for new degree program to catch any techinical errors from feedback\n",
    "\n",
    "Data Scalibility:\n",
    "\n",
    "Data Accesibility:\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day\n",
    " loaded into operational dashboard tool such as tableau\n",
    " * The database needed to be accessed by 100+ people.\n",
    " Each user will be given apporiate accesibility to the S3 bucket and Redshift cluster. Some may have only viewing access while other can have full access. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}